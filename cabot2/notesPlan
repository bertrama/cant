There are initially four nets for planning: goals1, facts, modules,
and actions. Later, when we introduce Roman's learning of turn toward
the stalactite, we will introduce a fifth, goals2, which is plastic.

There are several goal CAs, at least 10, and it's best to have at least
11 for Roman's goal.  The goals are largely turned on by
the commands. The first four are the simple commands, turn left,
right, move forward and backward.  These connect directly to the 
relevant module CAs, and are turned off by them (these can't go
wrong).  

The next two goals are move left and right and result in a
turn followed by a forward.  These are handled via the fact net. The
goal turns on ML1 (MR1). The fact net has two facts for this.  ML1
turn on the module left, and ML2.  ML2 turns on the module forward
and turns off the move left goal.  ML1 and ML2 are turned off by 
the forward and left modules.  

Aside: There is an ambiguity when the goals (go stalactite) and (turn
right) are on.  If the go says turn right, the extra right is ignored.
The use of (go stalactite) and (move forward) might help the
oscillation problem of CABot1.  (go stalactite) and (go pyramid) could
be an error or it could just go to the last one and erase the prior
goal.  The others should not be a problem as the goal should be completed
before the next goal is set.

The remaining four comands are (Turn toward (go to) the stalactite
(pyramid).)  These are handled with four goals, but further commands
like (push the cube) can lead to an exponential growth in commands
with a linear growth in goals.  The four goals are turn, go, pyramid
and stalactite.  A verb and noun must be activated (how is that
checked)?  The verb is turned on by the command, and the noun is half
turned on by the command, and half by the goal for the verb, and 
the verb continues to support it.

These four commands are of course quite complex.  First, the nouns
turn on facts asking, for example, where is the stalactite?  This
in turn send activation to the appropriate V2 areas which then connect
back to left right or centre facts.  The noun fact might also have 
a slowly building connection to the error2 fact (noun not present).

The left (right) fact turns on the left (right) module.  The modules
turn the facts off, and the turn goals.  The turn goal is needed to
support the noun fact so that will also be turned off.  The centre
fact along with the turn goal is connected to the error1 module
(turn toward centre). The centre fact and the go goal are connected
to the forward module.  

A final fact is needed to turn off go, when the object is big.  That
big fact needs to be supported by the second part of the big CAs.
Big V2 on along with noun fact, so Big fact comes on.  Big fact
turns of go goal. noun goal stops (without support from go), noun fact
turns off. Big V2 turns off. Big fact turns off.  This all requires a
lot of CAs that require an active CA to support them.


----------------------June 26---------------------
I've got a CABot2 that is a real improvement on CABot1.
It does the four primitives, 2 pairs, correctly every time I've tested.
It does the 2 context sensitive singles, and 2 context sensitive 
sequences correctly every time I've tested, but might fail on
the vision component. 

It can now cope with multiple items in the visual field. 

To do.
1. Integrate learning.
2. Catch errors: 
  a: turn toward non-existant object
  b: turn toward centre
3. Replace symbolic control with subsymbolic.

Note leave symbolic stp reset for CABot2.

The errors are now done.  The problem is that v2 often gets it wrong.  

The left right and center facts could be made partially support by the
objects so they turn off if necessary.

I've also now got largeV2 items coming on way too early or not at
all. It used to never come on as it was inhibited by the medium and
small states.  I removed that, and it comes on all the time.  See
setV2Topology.  Also the largeStalactite is really not included.
