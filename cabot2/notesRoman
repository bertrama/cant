Roman and I agreed on june 6th what would be in CABot2 with respect
to his (our) learning work.  We're going to have the agent know the
response for Turn toward the stalactite, but have it learn the
response for move toward the pyramid.  

Without this, I'll have a goal net, module net and fact net. With this 
I'll add another goal net to deal with this learning.  There will
be a goal (g0) in the original net, and 5 "subgoals" in the new net.  These
will refer to the position of the pyramid in the visual scene far left,
left, centre, right, far right.  These will then get hooked up to the 
value net and explore net so it works the right way.  

Roman is working on this now in isolation.

-----------------------------------------------------
I'm now (June 30th) about to do the practical bit of including the
goal-action learning stuff into CABot2.  On the up side this is quite 
early.  

My initial plan is to use 
Turn toward the stalactite. to set the goal.  If time allows later I'll use
Center the stalactite.
and switch the turn toward stalactite stuff back on.

I think I'm adding a scad of new nets.  I'll leave the existing goals
turn and stalactite (subnet goal1) but will use these to
activate a new centerStal goal.  centerStal will need to activate
the stall fact to invoke v2 to fill the the left right or center fact.

centerStal, and left or right should turn on one of two
goals in a new goal subnet (called goal2).
centerStal is switched off when the center fact comes on.

I will use a new module subnet (module2).  This will have just
left and right and learning is done between goal2 and module2.

The explore and value subnets are needed.  Active module2's turn off
explore.  Active goals turn it on. So initially there should be a
random process of picking actions for either goal that's set.

Value is set on by the center fact.  This should allow the connections
from the present goal2 to the last module2 to be reinforced.  Somehow
the goal2 and module2 will need to persist for some time and then
shut down.  I'm not sure about this somehow, but I could always put
a sequence net (like CABot1's Erase) to have it go for say 200 cycles.

Note that after the centerStal goal has stopped module2 should stop
igniting module1. That is there should be no more actions from this goal.


**************************
Ok, I've got it in.  The problem is that the moves are emitted too
quickly for feedback to be effective.  Also, two goals are being set.
**************************
It's in but there are a few problems.  
1. It works sometime, but there is no way to shut off value and 
the center-center fact. (This is mechanical.)
2. Sometimes it doesn't work.  If it takes a while to turn value on
then both left and right are turned on with a goal, leading to both
sets of weights increasing.  When value comes on, the weights from
goal2 still cause the module2 to switch from left to right and back.
3. Probably want to do something about a test. 

It looks like 2 and 1 is done.  In 1, the sequence net (value2) shuts
off goal2, value and fact.  

It seems that there is some sort of bug for the second request for
center the stalactite where the pictures don't get updated.  

To get the test going, let's make use of reset.  We'll loop through
 parse command center the stalactite
 center center fact on and value on
 measure first move
 center center fact off and value off
 measure rights and lefts
 reset crystal space
I put this in
********
It's starting to get there.  
It tends to learn the right thing.  
If it is in a good state it tends to stay in that state. But it can
  leave that state for two reasons. 1. It just goes on a walkabout,
  so if it is supposed to go right, it goes left and then takes a lot
  of moves (100s) to get back.  This means that the goal->left weights
  now are close to goal->right weights.  2. The value reinforces the
  wrong action.  Sometimes it is supposed to go right, and it does but
  by the time the value net comes up, it has gone left.  This problem
  can probably be reduced by making the modules persist a bit longer 
  than they arleady do.
On the up side, these two problems also mean that when it is in a bad
state, it can get back to a good state. 
It can also turn all the way around to get to the answer.  This also tends
to equalise the weights.




